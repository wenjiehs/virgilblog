---
title: 用 AI 搭建工作流自动化工具：三个小 Skills 的实践
date: 2026-02-10
tags: [AI编程, 产品实践, 工作流自动化, 学习笔记]
description: 产品经理用 AI 辅助开发三个工作流自动化工具的真实记录，包括需求追踪、周报生成、知识归档
---

# 用 AI 搭建工作流自动化工具：三个小 Skills 的实践

## 🤔 为什么要做这件事

做产品经理久了，会发现很多重复性工作其实可以自动化，但真要动手做时，又会因为"太麻烦"而放弃。

最近花了一些时间，尝试用 AI 辅助开发了几个自动化工具（基于内部的 AI Agent 平台），用来解决团队的一些日常痛点：
- 客户需求容易遗漏
- 写周报费时费力
- 产品知识散落各处

这篇文章不是技术炫耀，更像是一次复盘。分享一些过程中的思考、踩过的坑，以及这个方向的局限性。

**坦白说，这些都是小工具，技术含量不高，但确实省了些时间。**

## 🛠️ 做了什么

我基于公司内部的 AI Agent 平台，做了三个自动化工具：

### 1. 客户需求追踪系统

**要解决的问题**：

重要客户的需求经常散落在群聊里，容易遗漏。想要一个简单的登记系统，能自动提醒，还能每天早上推送未完成的需求清单。

**怎么做的**：

第一版超级简单：
- 用 Node.js 写了个 CRUD 系统
- 数据存本地 JSON 文件（是的，连数据库都没用）
- 支持企业微信群命令：`@Agent 登记需求：客户名 - 需求描述`
- 配置了定时任务，工作日早上 9 点自动推送

**后来的升级**：

本地文件不够安全，也不方便多人协作，所以又集成了公司内部文档系统作为存储后端。这样既有版本控制，也能多设备访问。

**现在的状态**：

功能能用，但说实话，还是有不少问题：
- ID 生成逻辑是按年份重置的（REQ-2026-001），跨年会有点尴尬
- 没有做任何性能优化，数据多了可能会慢
- 统计功能很基础，想看更复杂的报表还得手动算

但对我来说够用了——至少需求不会遗漏了。

### 2. 周报生成助手

**要解决的问题**：

写周报是个苦差事，尤其是要遵循团队的固定格式、避开各种敏感词、还要保持一致的风格。每次写都要对照上周的模板，很机械。

**实现思路**：

采用了"配置驱动"的方式：
- 把团队规则（敏感词、章节结构）写进配置文件
- 把团队知识（人员分工、术语表、案例）写进上下文文档
- 把历史周报模板作为参考示例
- AI 读取这些配置，生成符合规范的草稿

**工作流程**：

```
1. 投喂素材（会议记录、工作内容）
   ↓
2. AI 生成本地草稿文件
   ↓
3. 我人工确认修改
   ↓
4. 校验通过后发布到文档系统
```

**关键设计**：

这个系统的核心不是 AI 写得多好，而是"规则物理隔离但逻辑统一"。AI 生成时看的规则，和发布时校验的规则，是同一份配置文件。这样能避免"AI 写的东西过不了审"的尴尬。

**真实的局限**：
- 高度依赖团队规则的完整性，规则写得不好，输出也好不了
- AI 理解的"好文风"和人类理解的不完全一致，还是需要人工把关
- 对于创意性内容帮助不大，主要是减少格式化工作

### 3. 产品知识归档助手

**要解决的问题**：

产品逻辑、技术方案、排查经验经常口述后就忘了记录，或者记了也是散落各处的笔记，找起来很麻烦。

**实现方式**：

```
口述产品知识
  ↓
AI 识别类型（功能逻辑/架构设计/问题排查/产品决策）
  ↓
按不同模板整理成结构化文档
  ↓
自动搜索知识库，判断是新建还是补充已有文档
  ↓
打标签、建立文档关联，发布到文档系统
```

**典型场景**：

```
我："记录一下 GPU 直通功能。这个功能是通过 PCI passthrough 实现的..."

AI：
  - 识别为"功能逻辑类"
  - 创建新文档
  - 按模板整理
  - 打标签
  - 返回文档链接
```

**老实说的不足**：
- 文档质量高度依赖口述的完整性，说得不清楚，输出也乱
- 自动去重的相似度阈值是写死的（0.7），不够智能
- 标签提取还比较粗糙，需要人工补充
- 没有做知识图谱，文档间的关联还是线性的

**但它解决了一个关键问题**：降低了记录知识的门槛。以前懒得写文档，现在说一说就行。

## 💭 过程中的一些体会

### 1. 不要高估 AI 的能力

一开始我想让 AI 自动识别需求的优先级、自动分配负责人、自动关联项目管理系统。结果发现这些判断需要太多上下文，AI 经常猜错。

**后来的做法**：
- 优先级默认"高"，手动改
- 负责人默认是我，手动改
- 项目链接手动填

看起来很"笨"，但其实更实用。**自动化不是为了完全不动手，而是减少重复劳动。**

### 2. 配置比代码更重要

周报助手最初我想让 AI 记住团队规则，每次生成时"回忆"一下。但很快发现这不可靠——AI 的记忆会飘，同样的输入，输出可能不一致。

改成配置驱动后，规则变成了可追溯、可版本控制的文件。AI 只是个执行器，不需要"记住"任何东西。每次都重新读配置，反而更稳定。

### 3. 本地文件优于直接发布

最开始想直接把 AI 生成的内容推送到文档系统，后来发现一个问题：AI 生成的内容，我总想再看一眼再发出去。如果直接推送，就没有"反悔"的机会。

所以所有系统都改成了"先落地本地文件，确认后再发布"的模式。虽然多了一步，但心理压力小很多。

### 4. 简单比复杂好

需求系统本来想做成一个完整的项目管理工具，后来发现根本不需要。只要能登记、查询、提醒，就够用了。

很多功能是"想要"而不是"需要"。克制住做太多功能的冲动，反而能更快上线，更快迭代。

## 🔧 AI 辅助开发的真实体验

### AI 做得好的地方

1. **快速原型**：用自然语言描述需求，AI 能快速生成可运行的代码框架
2. **重复代码**：类似的 CRUD 操作、配置文件读写，AI 写得很快
3. **文档整理**：代码注释、README、配置说明，AI 能帮忙组织结构
4. **问题排查**：遇到报错时，AI 能快速给出可能的原因和解决方案

### AI 做不好的地方

1. **架构设计**：AI 倾向于给出"标准答案"，但实际场景往往需要权衡
2. **业务逻辑**：涉及具体业务规则时，AI 只能照你说的做，理解不了隐含的约束
3. **性能优化**：AI 生成的代码能跑，但不一定快，需要人工优化
4. **边界情况**：异常处理、边界条件，AI 经常遗漏

**总结就是**：AI 是个很快的初级工程师，能快速搭框架，但细节和判断力还是要人来把关。

## 🤔 一些观察

### 观察 1：规则的清晰度很重要

这些工具都依赖明确的规则。比如周报助手，规则写得详细，输出就比较符合预期；规则模糊，AI 就只能猜。

有些规则很难用文字表达清楚，比如"符合团队风格"，这时候就需要多提供几个历史样本让 AI 参考。

### 观察 2：结构化任务效果更好

登记需求、生成周报、整理文档这些任务比较模板化，AI 处理起来比较稳定。

但涉及到具体判断的地方（比如优先级评估、知识去重阈值），AI 容易出错，需要保留人工确认的环节。

### 观察 3：维护也是成本

这些工具用起来确实方便，但也需要持续维护：
- 规则要随着团队演进不断更新
- 配置文件要保持和实际情况同步
- 代码要跟着平台升级调整

我平均每周花 1-2 小时在维护上，主要是更新配置和修复小问题。

## 📊 实际效果

**确实带来的改变**：
- 需求遗漏情况明显减少
- 周报格式更统一
- 产品知识沉淀变得更容易

**开发时间**：
每个工具基本在 1 小时左右就能完成基础功能，后续根据使用情况逐步优化。

## 💡 如果你也想试试

### 1. 从最小可用版本开始

我的需求系统最初就是个 200 行的 Node.js 脚本，只能登记和查询。用了一段时间，发现确实有用，才慢慢加功能。

### 2. 把规则梳理清楚

花时间把团队规则、业务逻辑、异常情况整理成文档，比直接让 AI 生成代码重要。规则越清晰，AI 输出越稳定。

### 3. 保留人工确认环节

在关键决策点，留一个人工确认的步骤。我的系统都是先生成本地文件，确认后再发布，避免出错。

### 4. 配置和代码分离

规则、知识、模板尽量放在配置文件里，不要硬编码。这样规则变化时只需要改配置，不需要改代码。

## 🎉 小结

这次实践让我对 AI 辅助工作有了更具体的体会。

AI 在快速搭框架、处理重复工作、整理结构化内容上确实有帮助，但关键判断和业务逻辑还是需要人来把关。

这些工具的主要价值：
- ✅ 把团队规则、产品知识变成了可维护的文件
- ✅ 把一些重复性工作自动化了
- ✅ 节省了一些时间，让团队能关注更重要的事

代码质量一般（没有单元测试、错误处理粗糙、没做性能优化），但解决了实际问题，够用就好。

---

## 📝 关于这篇文章

这篇文章基于真实的实践总结。文章初稿用 AI 辅助整理了结构，但核心内容和观点来自实际开发过程中的记录。

*希望对你有参考价值。* 😊
